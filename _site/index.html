<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="icon" href="/assets/favicon.svg" type="image/svg+xml" />
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
<link rel="stylesheet" href="/assets/css/main.css" />
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Home | Koki Maeda</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Home" />
<meta property="og:locale" content="en" />
<meta name="description" content="Doctoral student exploring multimodal vision-and-language systems, evaluation metrics, and context-aware captioning." />
<meta property="og:description" content="Doctoral student exploring multimodal vision-and-language systems, evaluation metrics, and context-aware captioning." />
<link rel="canonical" href="https://silviase.github.io/" />
<meta property="og:url" content="https://silviase.github.io/" />
<meta property="og:site_name" content="Koki Maeda" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Home" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Doctoral student exploring multimodal vision-and-language systems, evaluation metrics, and context-aware captioning.","headline":"Home","name":"Koki Maeda","url":"https://silviase.github.io/"}</script>
<!-- End Jekyll SEO tag -->


  </head>
  <body class="layout-home">
    <div class="page-shell">
      <header class="site-header">
  <div class="container">
    <div class="site-branding">
      <a class="site-title" href="/">Koki Maeda</a>
      
      <p class="site-tagline">Doctoral student exploring multimodal vision-and-language systems, evaluation metrics, and context-aware captioning.</p>
      
    </div>
    <nav class="site-nav">
           
      <a href="/" class="active">Home</a>
         
      <a href="/cv/" class="">CV</a>
         
      <a href="/papers/" class="">Papers</a>
         
      <a href="/blog/" class="">Blog</a>
      
    </nav>
  </div>
</header>

      <main class="site-main">




<section class="hero">
  <div class="container hero-grid">
    <div class="hero-image">
      <img src="/assets/face/202503.png" alt="Koki Maeda" loading="lazy" />
    </div>
    <div class="hero-content">
      <h1>Koki Maeda</h1>
      <p class="hero-tagline">Doctoral Student ¬∑ Vision and Language Researcher</p>
      <p class="hero-bio">Exploring the intersection of computer vision and natural language processing, with a focus on multimodal evaluation metrics and context-aware image captioning.</p>
      <div class="hero-links">
        
          
          
          <a href="mailto:koki.maeda@nlp.c.titech.ac.jp">
            
              <span class="hero-link-icon" aria-hidden="true">
                üìß
                  
              </span>
            
            <span class="hero-link-label">Email</span>
          </a>
        
          
          
          <a href="https://scholar.google.com/citations?user=TOHpU1IAAAAJ">
            
              <span class="hero-link-icon" aria-hidden="true">
                üéì
                  
              </span>
            
            <span class="hero-link-label">Google Scholar</span>
          </a>
        
          
          
          <a href="https://github.com/Silviase">
            
              <span class="hero-link-icon" aria-hidden="true">
                üêô
                  
              </span>
            
            <span class="hero-link-label">GitHub</span>
          </a>
        
          
          
          <a href="https://linkedin.com/in/silviase">
            
              <span class="hero-link-icon" aria-hidden="true">
                üíº
                  
              </span>
            
            <span class="hero-link-label">LinkedIn</span>
          </a>
        
          
          
          <a href="/blog/">
            
              <span class="hero-link-icon" aria-hidden="true">
                üìù
                  
              </span>
            
            <span class="hero-link-label">Blog</span>
          </a>
        
          
          
          <a href="/cv/">
            
              <span class="hero-link-icon" aria-hidden="true">
                üìÑ
                  
              </span>
            
            <span class="hero-link-label">CV</span>
          </a>
        
      </div>
    </div>
  </div>
</section>


<section class="page-section research">
  <div class="container">
    <h2>Research Themes</h2>
    <div class="pill-grid">
      
      <article class="pill-card">
        <h3>Multimodal Evaluation & Metrics</h3>
        <p>Building reliable evaluation protocols for vision-language models, with a focus on grounding and cultural nuance in Japanese contexts.</p>
      </article>
      
      <article class="pill-card">
        <h3>Instruction-Tuned Multimodal LLMs</h3>
        <p>Curating instruction datasets and training recipes that keep open-weight models aligned and deployable.</p>
      </article>
      
      <article class="pill-card">
        <h3>Structured Generation & Understanding</h3>
        <p>Teaching models to produce and consume structured outputs such as captions, diagrams, and graphs for real-world tasks.</p>
      </article>
      
    </div>
  </div>
</section>


<section class="page-section publications">
  <div class="container">
    <h2>Featured Publications</h2>
    <div class="paper-list condensed">
      
        
          <article class="paper-card">
  <h3 class="paper-title">
    <a href="/papers/2024-comkitchens/">COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset as a Vision-Language Benchmark</a>
  </h3>
  <p class="paper-venue">
     Proceedings of The 18th European Conference on Computer Vision (ECCV 2024)  
    ¬∑ 2024 
  </p>
  
  <p class="paper-description">Introducing a new vision-language dataset based on unedited overhead-view procedural cooking videos.</p>
  
  <p class="paper-authors">
    
    
    <span class="paper-author"><strong><u>Koki Maeda</u></strong></span>,  
    
    <span class="paper-author">Tosho Hirasawa</span>,  
    
    <span class="paper-author">Atsushi Hashimoto</span>,  
    
    <span class="paper-author">Jun Harashima</span>,  
    
    <span class="paper-author">Leszek Rybicki</span>,  
    
    <span class="paper-author">Yusuke Fukasawa</span>,  
    
    <span class="paper-author">Yoshitaka Ushiku</span> 
  </p>
  <div class="paper-links">
       
    <a class="paper-link" href="https://doi.org/10.32130/rdata.6.1" target="_blank" rel="noopener">PDF</a>
     
    <a class="paper-link" href="https://github.com/omron-sinicx/com_kitchens" target="_blank" rel="noopener">Code</a>
     
    <button class="paper-link copy-bibtex" data-bibtex="@inproceedings{maeda2024comkitchens,
  title = {COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset as a Vision-Language Benchmark},
  author = {Koki Maeda and Tosho Hirasawa and Atsushi Hashimoto and Jun Harashima and Leszek Rybicki and Yusuke Fukasawa and Yoshitaka Ushiku},
  booktitle = {Proceedings of The 18th European Conference on Computer Vision (ECCV 2024)},
  year = {2024},
  address = {Milan, Italy},
  publisher = {ECCV}
}">Copy BibTeX</button>
    
  </div>
</article>

        
          <article class="paper-card">
  <h3 class="paper-title">
    <a href="/papers/2023-quic360/">Query-based Image Captioning from Multi-context 360-degree Images</a>
  </h3>
  <p class="paper-venue">
     Findings of the Association for Computational Linguistics: EMNLP 2023  
    ¬∑ 2023 
  </p>
  
  <p class="paper-description">A novel image captioning approach that leverages queries and multi-context 360-degree imagery.</p>
  
  <p class="paper-authors">
    
    
    <span class="paper-author"><strong><u>Koki Maeda</u></strong></span>,  
    
    <span class="paper-author">Shuhei Kurita</span>,  
    
    <span class="paper-author">Taiki Miyanishi</span>,  
    
    <span class="paper-author">Naoaki Okazaki</span> 
  </p>
  <div class="paper-links">
       
    <a class="paper-link" href="https://aclanthology.org/2023.findings-emnlp.463.pdf" target="_blank" rel="noopener">PDF</a>
     
    <a class="paper-link" href="https://github.com/Silviase/QuIC-360" target="_blank" rel="noopener">Code</a>
     
    <button class="paper-link copy-bibtex" data-bibtex="@inproceedings{maeda2023quic360,
  title = {Query-based Image Captioning from Multi-context 360-degree Images},
  author = {Koki Maeda and Shuhei Kurita and Taiki Miyanishi and Naoaki Okazaki},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages = {6940--6954},
  year = {2023},
  address = {Singapore},
  publisher = {Association for Computational Linguistics}
}">Copy BibTeX</button>
    
  </div>
</article>

        
          <article class="paper-card">
  <h3 class="paper-title">
    <a href="/papers/2022-impara/">IMPARA: Impact-Based Metric for GEC Using Parallel Data</a>
  </h3>
  <p class="paper-venue">
     Proceedings of the 29th International Conference on Computational Linguistics (COLING 2022)  
    ¬∑ 2022 
  </p>
  
  <p class="paper-description">Proposal of a new impact-based metric for grammatical error correction using parallel datasets.</p>
  
  <p class="paper-authors">
    
    
    <span class="paper-author"><strong><u>Koki Maeda</u></strong></span>,  
    
    <span class="paper-author">Masahiro Kaneko</span>,  
    
    <span class="paper-author">Naoaki Okazaki</span> 
  </p>
  <div class="paper-links">
       
    <a class="paper-link" href="https://aclanthology.org/2022.coling-1.316.pdf" target="_blank" rel="noopener">PDF</a>
     
    <a class="paper-link" href="https://github.com/Silviase/IMPARA" target="_blank" rel="noopener">Code</a>
     
    <button class="paper-link copy-bibtex" data-bibtex="@inproceedings{maeda2022impara,
  title = {IMPARA: Impact-Based Metric for GEC Using Parallel Data},
  author = {Koki Maeda and Masahiro Kaneko and Naoaki Okazaki},
  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics (COLING 2022)},
  pages = {3578--3588},
  year = {2022},
  address = {Gyeongju, Republic of Korea},
  publisher = {International Committee on Computational Linguistics}
}">Copy BibTeX</button>
    
  </div>
</article>

        
      
    </div>
  </div>
</section>

<section class="page-section publications">
  <div class="container">
    <h2>Recent Publications</h2>
    <div class="paper-list condensed">
      
      
        
          <article class="paper-card">
  <h3 class="paper-title">
    <a href="/papers/2025-whywe/">Why We Build Local Large Language Models: An Observational Analysis from 35 Japanese and Multilingual LLMs</a>
  </h3>
  <p class="paper-venue">
     The 1st Workshop on Multilingual and Equitable Language Technologies (MELT)  
    ¬∑ 2025 
  </p>
  
  <p class="paper-authors">
    
    
    <span class="paper-author">Koshiro Saito</span>,  
    
    <span class="paper-author">Sakae Mizuki</span>,  
    
    <span class="paper-author">Masanari Ohi</span>,  
    
    <span class="paper-author">Taishi Nakamura</span>,  
    
    <span class="paper-author">Taihei Shiotani</span>,  
    
    <span class="paper-author"><strong><u>Koki Maeda</u></strong></span>,  
    
    <span class="paper-author">Youmi Ma</span>,  
    
    <span class="paper-author">Kakeru Hattori</span>,  
    
    <span class="paper-author">Kazuki Fujii</span>,  
    
    <span class="paper-author">Takumi Okamoto</span>,  
    
    <span class="paper-author">Shigeki Ishida</span>,  
    
    <span class="paper-author">Hiroya Takamura</span>,  
    
    <span class="paper-author">Rio Yokota</span>,  
    
    <span class="paper-author">Naoaki Okazaki</span> 
  </p>
  <div class="paper-links">
       
    <button class="paper-link copy-bibtex" data-bibtex="@inproceedings{}">Copy BibTeX</button>
    
  </div>
</article>

          
        
      
        
          <article class="paper-card">
  <h3 class="paper-title">
    <a href="/papers/2025-instruction-tuning/">Building instruction-tuning datasets from human-written instructions with open-weight large language models</a>
  </h3>
  <p class="paper-venue">
     The 2nd Conference on Language Modeling (COLM)  
    ¬∑ 2025 
  </p>
  
  <p class="paper-authors">
    
    
    <span class="paper-author">Youmi Ma</span>,  
    
    <span class="paper-author">Sakae Mizuki</span>,  
    
    <span class="paper-author">Kazuki Fujii</span>,  
    
    <span class="paper-author">Taishi Nakamura</span>,  
    
    <span class="paper-author">Masanari Ohi</span>,  
    
    <span class="paper-author">Hinari Shimada</span>,  
    
    <span class="paper-author">Taihei Shiotani</span>,  
    
    <span class="paper-author">Koshiro Saito</span>,  
    
    <span class="paper-author"><strong><u>Koki Maeda</u></strong></span>,  
    
    <span class="paper-author">Kakeru Hattori</span>,  
    
    <span class="paper-author">Takumi Okamoto</span>,  
    
    <span class="paper-author">Shigeki Ishida</span>,  
    
    <span class="paper-author">Rio Yokota</span>,  
    
    <span class="paper-author">Hiroya Takamura</span>,  
    
    <span class="paper-author">Naoaki Okazaki</span> 
  </p>
  <div class="paper-links">
       
    <button class="paper-link copy-bibtex" data-bibtex="@inproceedings{}">Copy BibTeX</button>
    
  </div>
</article>

          
        
      
        
          <article class="paper-card">
  <h3 class="paper-title">
    <a href="/papers/2025-llmjp3vila/">Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model</a>
  </h3>
  <p class="paper-venue">
     Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)  
    ¬∑ 2025 
  </p>
  
  <p class="paper-description">Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models.</p>
  
  <p class="paper-authors">
    
    
    <span class="paper-author">Keito Sasagawa</span>,  
    
    <span class="paper-author"><strong><u>Koki Maeda</u></strong></span>,  
    
    <span class="paper-author">Issa Sugiura</span>,  
    
    <span class="paper-author">Shuhei Kurita</span>,  
    
    <span class="paper-author">Naoaki Okazaki</span>,  
    
    <span class="paper-author">Daisuke Kawahara</span> 
  </p>
  <div class="paper-links">
       
    <a class="paper-link" href="https://arxiv.org/pdf/2410.22736" target="_blank" rel="noopener">PDF</a>
     
    <a class="paper-link" href="https://huggingface.co/llm-jp/llm-jp-3-vila-14b" target="_blank" rel="noopener">Code</a>
     
    <button class="paper-link copy-bibtex" data-bibtex="@inproceedings{sasagawa2025llmjp3vila,
  title = {Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model},
  author = {Keito Sasagawa and Koki Maeda and Issa Sugiura and Shuhei Kurita and Naoaki Okazaki and Daisuke Kawahara},
  booktitle = {Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)},
  year = {2025},
  address = {Albuquerque, USA},
  publisher = {Association for Computational Linguistics}
}">Copy BibTeX</button>
    
  </div>
</article>

          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="section-cta">
      <a class="button ghost" href="/papers/">Browse full publication list</a>
    </div>
  </div>
</section>



<section class="page-section talks">
  <div class="container">
    <h2>Talks &amp; Workshops</h2>
    <ol class="talk-list">
      
        
        
        
        
        <li>
          
            <a href="https://llmc.nii.ac.jp/llmc_sympo2025/">Ë©ï‰æ°„ÅÆË¶≥ÁÇπ„Åã„ÇâË¶ã„ÇãÂõΩÁî£VLM„ÅÆÁèæÁä∂</a>
          
          
            <span class="talk-meta"> ¬∑ Japanese Symposium on Open Large Language Models ¬∑ Tokyo, Japan ¬∑ 2025-11-26</span>
          
        </li>
      
    </ol>
  </div>
</section>


<section class="page-section education">
  <div class="container">
    <h2>Education</h2>
    <div class="timeline">
      
      <article class="timeline-item">
        <p class="timeline-years">2024 ‚Äì Present</p>
        <h3 class="timeline-title">Ph.D., Tokyo Institute of Technology</h3>
        <p class="timeline-focus">Vision and Language, Evaluation</p>
        
        <p class="timeline-advisors">
          <strong>Advisors:</strong>
           Naoaki Okazaki 
        </p>
        
        <p class="timeline-description">Focusing on novel evaluation metrics for multimodal systems and cross-modal representation learning.</p>
      </article>
      
      <article class="timeline-item">
        <p class="timeline-years">2022 ‚Äì 2024</p>
        <h3 class="timeline-title">M.Eng., Tokyo Institute of Technology</h3>
        <p class="timeline-focus">Vision and Language: Image Captioning</p>
        
        <p class="timeline-advisors">
          <strong>Advisors:</strong>
           Naoaki Okazaki 
        </p>
        
        <p class="timeline-description">Developed context-aware image captioning models that generate descriptions based on user preferences.</p>
      </article>
      
      <article class="timeline-item">
        <p class="timeline-years">2018 ‚Äì 2022</p>
        <h3 class="timeline-title">B.Eng., Tokyo Institute of Technology</h3>
        <p class="timeline-focus">NLP: Grammatical Error Correction</p>
        
        <p class="timeline-advisors">
          <strong>Advisors:</strong>
           Naoaki Okazaki,   Masahiro Kaneko (Mentor) 
        </p>
        
        <p class="timeline-description">Created improved evaluation metrics for grammatical error correction systems.</p>
      </article>
      
    </div>
  </div>
</section>

<section class="page-section awards">
  <div class="container">
    <h2>Awards &amp; Fellowships</h2>
    <ul class="badge-list">
      
      <li class="badge">Young Scientist Award, ANLP (2025)</li>
      
      <li class="badge">Committee Special Awarded Paper, ANLP (2025, 2023)</li>
      
      <li class="badge">Program for Development of Co-creative Experts towards Top-level AI Research (Science Tokyo BOOST) for Science and Engineering fields (2024‚Äì2027)</li>
      
      <li class="badge">Awarded Paper, ANLP (2022)</li>
      
    </ul>
  </div>
</section>

<section class="page-section skills">
  <div class="container">
    <h2>Skills &amp; Expertise</h2>
    <div class="skills-grid">
      
      <article class="skill-group">
        <h3>Programming</h3>
        <ul>
          
          <li>Python</li>
          
          <li>PyTorch</li>
          
          <li>Java</li>
          
        </ul>
      </article>
      
      <article class="skill-group">
        <h3>Research Areas</h3>
        <ul>
          
          <li>Computer Vision</li>
          
          <li>Natural Language Processing</li>
          
          <li>Multimodal Learning</li>
          
          <li>Image Captioning</li>
          
          <li>Evaluation Metrics</li>
          
        </ul>
      </article>
      
      <article class="skill-group">
        <h3>Languages</h3>
        <ul>
          
          <li>Japanese (Native)</li>
          
          <li>English (Professional)</li>
          
        </ul>
      </article>
      
    </div>
  </div>
</section>
</main>
      <footer class="site-footer">
  <div class="container">
    <p>¬© 2025 Koki Maeda ¬∑ Built with Jekyll</p>
  </div>
</footer>

    </div>
    <script src="/assets/js/site.js"></script>
    
  </body>
</html>
