[
  
  

  
  {
    "id": null,
    "title": "Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model",
    "venue": "Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)",
    "description": "Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models.",
    "pdf_link": "https://arxiv.org/pdf/2410.22736",
    "code_link": "https://huggingface.co/llm-jp/llm-jp-3-vila-14b",
    "type": "international",
    "bibtex": "@inproceedings{sasagawa2025llmjp3vila,\n  title = {Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model},\n  author = {Keito Sasagawa and Koki Maeda and Issa Sugiura and Shuhei Kurita and Naoaki Okazaki and Daisuke Kawahara},\n  booktitle = {Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)},\n  year = {2025},\n  address = {Albuquerque, USA},\n  publisher = {Association for Computational Linguistics}\n}\n",
    "url": "/papers/2025-llmjp3vila/"
  }
  
  ,
  
  {
    "id": null,
    "title": "LegalViz: Legal Text Visualization by Text To Diagram Generation",
    "venue": "Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    "description": "Novel approach for visualizing complex legal document structures through diagram generation from text.",
    "pdf_link": "https://arxiv.org/pdf/2502.06147",
    "code_link": "",
    "type": "international",
    "bibtex": "@inproceedings{onami2024legalviz,\n  title = {LegalViz: Legal Text Visualization by Text To Diagram Generation},\n  author = {Eri Onami and Taiki Miyanishi and Koki Maeda and Shuhei Kurita},\n  booktitle = {Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  year = {2025},\n  address = {Albuquerque, USA},\n  publisher = {Association for Computational Linguistics}\n}\n",
    "url": "/papers/2025-legalviz/"
  }
  
  ,
  
  {
    "id": null,
    "title": "Swallowコーパスv2: 教育的な日本語ウェブコーパスの構築",
    "venue": "言語処理学会第31回年次大会 (NLP2025)",
    "description": "大規模言語モデルの学習には高品質なコーパスが不可欠だが、日本語では特に教育的価値の高いテキストデータが不足している。本研究では、ウェブから収集した日本語テキストに対して教育的価値を評価し、高品質なコーパスを構築する手法を提案する。FastTextとLLMを組み合わせた効率的なフィルタリングパイプラインを開発し、3540億トークンから精選した350億トークンのSwallowコーパスv2を構築した。本コーパスで学習したモデルは、既存の日本語ベンチマークで優れた性能を示し、特に推論・知識タスクで顕著な改善を達成した。",
    "pdf_link": "/assets/papers/2025_d_swallow_corpus_v2.pdf",
    "code_link": "https://github.com/swallow-llm/swallow-corpus",
    "type": "domestic",
    "bibtex": "@inproceedings{maeda2025swallow,\n  title={Swallowコーパスv2: 教育的な日本語ウェブコーパスの構築},\n  author={服部 翔 and 岡崎 直観 and 水木 栄 and 藤井 一喜 and 中村 泰士 and 大井 聖也 and 塩谷 泰平 and 齋藤 幸史郎 and Youmi Ma and 前田 航希 and 岡本 拓己 and 石田 茂樹 and 横田 理央 and 高村 大也},\n  booktitle={言語処理学会第31回年次大会 (NLP2025)},\n  year={2025},\n  address={長崎}\n}\n",
    "url": "/papers/2025-d-swallow-corpus-v2/"
  }
  
  ,
  
  {
    "id": null,
    "title": "新聞記事からつくる時事と社会に強い日本語LLM",
    "venue": "言語処理学会第31回年次大会 (NLP2025)",
    "description": "大規模言語モデル（LLM）の学習データは主にウェブから収集されるため、時事問題や社会的な知識の習得に課題がある。本研究では、新聞記事データを活用してLLMを継続事前学習することで、時事・社会問題に関する知識と理解力を向上させる手法を提案する。具体的には、朝日新聞社の記事データを用いてLLMを継続学習し、時事問題に関するベンチマークで既存モデルを上回る性能を達成した。また、新聞特有の文体や表現を適切に扱うための前処理手法も開発した。",
    "pdf_link": "/assets/papers/2025_d_news_llm.pdf",
    "code_link": "",
    "type": "domestic",
    "bibtex": "@inproceedings{maeda2025news,\n  title={新聞記事からつくる 時事と社会に強い日本語LLM},\n  author={服部 翔 and 水木 栄 and 藤井 一喜 and 中村 泰士 and 塩谷 泰平 and 植木 快 and 新妻 巧朗 and 川畑 輝 and 田森 秀明 and Youmi Ma and 前田 航希 and 大井 聖也 and 齋藤 幸史郎 and 岡本 拓己 and 石田 茂樹 and 横田 理央 and 高村 大也 and 岡崎 直観},\n  booktitle={言語処理学会第31回年次大会 (NLP2025)},\n  year={2025},\n  address={長崎}\n}\n",
    "url": "/papers/2025-d-news-llm/"
  }
  
  ,
  
  {
    "id": null,
    "title": "llm-jp-eval-mm: 日本語視覚言語モデルの自動評価基盤",
    "venue": "言語処理学会第31回年次大会 (NLP2025)",
    "description": "Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models.",
    "pdf_link": "/assets/papers/2025_d_llmjpevalmm.pdf",
    "code_link": "https://github.com/llm-jp/llm-jp-eval-mm",
    "type": "domestic",
    "bibtex": "@inproceedings{maeda2025llm-jp-eval-mm,\n  author = {前田, 航希 and 杉浦, 一瑳 and 小田, 悠介 and 栗田, 修平 and 岡崎, 直観},\n  month = mar,\n  series = {言語処理学会第31回年次大会 (NLP2025)},\n  title = {{llm-jp-eval-mm: 日本語視覚言語モデルの自動評価基盤}},\n  year = {2025}\n}\n",
    "url": "/papers/2025-d-llmjpevalmm/"
  }
  
  ,
  
  {
    "id": null,
    "title": "LLM-jp-3 VILA: 日本語マルチモーダルデータセット及び強力な日本語マルチモーダルモデルの構築",
    "venue": "言語処理学会第31回年次大会 (NLP2025)",
    "description": "非英語圏における視覚言語モデル（VLM）の開発は、高品質なマルチモーダルデータセットの不足により大きく制限されている。本研究では、日本語VLMの迅速な開発を可能にする包括的なマルチモーダルデータセット構築手法を提案する。ウェブアーカイブから600万件の日本語画像-テキストペアを抽出し、既存VLMを活用して画像から直接36.9万件の指示データを生成した。構築したLLM-jp-3 VILA 14Bは、日本語マルチモーダルベンチマークにおいて最先端の性能を達成し、機械翻訳データに頼る従来手法の限界を克服した。",
    "pdf_link": "/assets/papers/2025_d_llmjp3vila.pdf",
    "code_link": "https://huggingface.co/llm-jp/llm-jp-3-vila-14b",
    "type": "domestic",
    "bibtex": "@inproceedings{sasagawa2025llmjp3vila_domestic,\n  author = {笹川 慶人 and 前田 航希 and 杉浦 一瑳 and 栗田 修平 and 岡崎 直観 and 河原 大輔},\n  title = {LLM-jp-3 VILA: 日本語マルチモーダルデータセット及び強力な日本語マルチモーダルモデルの構築},\n  booktitle = {言語処理学会第31回年次大会 (NLP2025)},\n  year = {2025},\n  month = mar,\n  address = {長崎}\n}\n",
    "url": "/papers/2025-d-llmjp3vila/"
  }
  
  ,
  
  {
    "id": null,
    "title": "多言語での判例事実概要からの法的関係性のグラフ可視化",
    "venue": "言語処理学会第31回年次大会 (NLP2025)",
    "description": "法的文書の理解には専門知識が必要であり、一般市民にとって大きな障壁となっている。本研究では、判例事実概要から法的関係性を抽出し、グラフとして可視化する手法を提案する。大規模言語モデルを活用して法的エンティティと関係性を抽出し、構造化されたグラフ表現に変換することで、複雑な法的関係を直感的に理解可能にする。多言語（日本語・英語）での評価実験により、提案手法が法的文書の構造を適切に把握し、可視化できることを確認した。法教育や法的アクセスの向上に貢献する技術として期待される。",
    "pdf_link": "/assets/papers/2025_d_legalviz.pdf",
    "code_link": "",
    "type": "domestic",
    "bibtex": "@inproceedings{onami2025legalviz,\n  author = {大南 英理 and 宮西 大樹 and 前田 航希 and 栗田 修平},\n  title = {多言語での判例事実概要からの法的関係性のグラフ可視化},\n  booktitle = {言語処理学会第31回年次大会 (NLP2025)},\n  year = {2025},\n  month = mar,\n  address = {長崎}\n}\n",
    "url": "/papers/2025-d-legalviz/"
  }
  
  ,
  
  {
    "id": null,
    "title": "模倣学習による大規模言語モデルの指示チューニング",
    "venue": "言語処理学会第31回年次大会 (NLP2025)",
    "description": "大規模言語モデル（LLM）の指示チューニングにおいて、高品質な指示応答データの作成は重要な課題である。本研究では、強力なLLMの出力を模倣することで、効率的に高品質な指示チューニングデータを構築する手法を提案する。具体的には、GPT-4等の先進的なモデルに対して多様な指示を与え、その応答を収集・精選することで、日本語LLMの性能向上を実現した。実験では、模倣学習により作成したデータで学習したモデルが、従来手法を上回る性能を示すことを確認した。",
    "pdf_link": "/assets/papers/2025_d_imitation_learning.pdf",
    "code_link": "https://swallow-llm.github.io/llama3.1-swallow.ja",
    "type": "domestic",
    "bibtex": "@inproceedings{maeda2025imitation,\n  title={模倣学習による大規模言語モデルの指示チューニング},\n  author={Youmi Ma and 水木 栄 and 藤井 一喜 and 中村 泰士 and 大井 聖也 and 島田 比奈理 and 塩谷 泰平 and 齋藤 幸史郎 and 前田 航希 and 服部 翔 and 岡本 拓己 and 石田 茂樹 and 横田 理央 and 高村 大也 and 岡崎 直観},\n  booktitle={言語処理学会第31回年次大会 (NLP2025)},\n  year={2025},\n  address={長崎}\n}\n",
    "url": "/papers/2025-d-imitation-learning/"
  }
  
  ,
  
  {
    "id": null,
    "title": "視覚的文脈を利用した視覚言語モデルによる画像キャプション生成自動評価手法",
    "venue": "言語処理学会第30回年次大会 (NLP2024)",
    "description": "画像キャプション生成の自動評価において、従来手法は画像と生成キャプションの意味的整合性のみを考慮し、文脈情報を無視していた。本研究では、視覚言語モデルを用いて画像の視覚的文脈を考慮した新たな自動評価手法を提案する。提案手法は、画像中の物体間の関係性や空間的配置、シーンの文脈を理解し、それらを評価に反映させることで、人間の評価により近い結果を実現する。実験により、提案手法が既存の評価指標よりも人間の判断との相関が高いことを示した。",
    "pdf_link": "/assets/papers/2024_d_visual_context_caption.pdf",
    "code_link": "https://github.com/Silviase/VisCE2",
    "type": "domestic",
    "bibtex": "@inproceedings{maeda2024visual,\n  title={視覚的文脈を利用した視覚言語モデルによる画像キャプション生成自動評価手法},\n  author={前田 航希 and 栗田 修平 and 宮西 大樹 and 岡崎 直観},\n  booktitle={言語処理学会第30回年次大会 (NLP2024)},\n  pages={1996--2001},\n  year={2024},\n  address={東京}\n}\n",
    "url": "/papers/2024-d-visual-context-caption/"
  }
  
  ,
  
  {
    "id": null,
    "title": "調理作業理解のための言語資源付き固定視点映像データセットの構築",
    "venue": "2024年度人工知能学会全国大会（第38回）",
    "description": "既存の調理映像データセットは主にウェブ動画や一人称視点に限定され、実際の調理環境の多様性を反映していない。本研究では、無編集の俯瞰視点調理映像と詳細な言語アノテーションを含む新たなデータセットCOM Kitchensを構築した。スマートフォンで撮影した多様な環境での調理映像に対し、視覚的行動グラフによる構造的アノテーションを付与し、オンラインレシピ検索と密な映像キャプション生成という2つの新しいタスクを提案。既存モデルによる評価実験により、本データセットが視覚言語理解の新たな課題を提供することを示した。",
    "pdf_link": "/assets/papers/2024_d_comkitchens.pdf",
    "code_link": "https://github.com/omron-sinicx/com_kitchens",
    "type": "domestic",
    "bibtex": "@inproceedings{hashimoto2024comkitchens,\n  title={調理作業理解のための言語資源付き固定視点映像データセットの構築},\n  author={橋本 敦史 and 前田 航希 and 平澤 寅庄 and 原島 純 and RYBICKI Leszek and 深澤 祐援 and 牛久 祥孝},\n  booktitle={2024年度人工知能学会全国大会（第38回）},\n  year={2024},\n  address={浜松}\n}\n",
    "url": "/papers/2024-d-comkitchens/"
  }
  
  ,
  
  {
    "id": null,
    "title": "COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset as a Vision-Language Benchmark",
    "venue": "Proceedings of The 18th European Conference on Computer Vision (ECCV 2024)",
    "description": "Introducing a new vision-language dataset based on unedited overhead-view procedural cooking videos.",
    "pdf_link": "https://doi.org/10.32130/rdata.6.1",
    "code_link": "https://github.com/omron-sinicx/com_kitchens",
    "type": "international",
    "bibtex": "@inproceedings{maeda2024comkitchens,\n  title = {COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset as a Vision-Language Benchmark},\n  author = {Koki Maeda and Tosho Hirasawa and Atsushi Hashimoto and Jun Harashima and Leszek Rybicki and Yusuke Fukasawa and Yoshitaka Ushiku},\n  booktitle = {Proceedings of The 18th European Conference on Computer Vision (ECCV 2024)},\n  year = {2024},\n  address = {Milan, Italy},\n  publisher = {ECCV}\n}\n",
    "url": "/papers/2024-comkitchens/"
  }
  
  ,
  
  {
    "id": null,
    "title": "Query-based Image Captioning from Multi-context 360-degree Images",
    "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
    "description": "A novel image captioning approach that leverages queries and multi-context 360-degree imagery.",
    "pdf_link": "https://aclanthology.org/2023.findings-emnlp.463.pdf",
    "code_link": "https://github.com/Silviase/QuIC-360",
    "type": "international",
    "bibtex": "@inproceedings{maeda2023quic360,\n  title = {Query-based Image Captioning from Multi-context 360-degree Images},\n  author = {Koki Maeda and Shuhei Kurita and Taiki Miyanishi and Naoaki Okazaki},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},\n  pages = {6940--6954},\n  year = {2023},\n  address = {Singapore},\n  publisher = {Association for Computational Linguistics}\n}\n",
    "url": "/papers/2023-quic360/"
  }
  
  ,
  
  {
    "id": null,
    "title": "DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning",
    "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)",
    "description": "Vision-language models like CLIP show strong zero-shot performance but struggle when fine-tuned on downstream tasks due to overfitting. This paper proposes DueT (Dual-adapter Tuning), which uses separate adapters for uni-modal and cross-modal features to prevent overfitting while maintaining the pre-trained knowledge. The method introduces contrastive learning between adapted and original features, achieving state-of-the-art results on multiple vision-language benchmarks. DueT demonstrates significant improvements over existing adapter-based methods, particularly in few-shot scenarios where overfitting is most problematic.",
    "pdf_link": "https://aclanthology.org/2023.emnlp-main.839.pdf",
    "code_link": "",
    "type": "international",
    "bibtex": "@inproceedings{hasegawa2023duet,\n  title = {DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning},\n  author = {Taku Hasegawa and Kyosuke Nishida and Koki Maeda and Kuniko Saito},\n  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)},\n  pages = {13607--13624},\n  year = {2023},\n  address = {Singapore},\n  publisher = {Association for Computational Linguistics}\n}\n",
    "url": "/papers/2023-duet/"
  }
  
  ,
  
  {
    "id": null,
    "title": "QuIC-360◦: 360◦ 画像に対するクエリ指向画像説明文生成のためのデータセット構築",
    "venue": "言語処理学会第29回年次大会 (NLP2023)",
    "description": "360◦ 画像は一般的な画像と比較して，撮影者による情報の取捨選択が行われないため，多くのコンテクストを同時に含む．既存の画像説明文生成では，コンテクストを画像情報のみから読み取るが，360◦ 画像に対しては，画像に加えて補助的な情報を付加することで，記述するコンテクストを指定することが必要になる．本研究では，画像に加えて言語情報（クエリ）を与えることで説明文生成を制御するクエリ指向説明文生成を提案し，そのためのデータセットとして 5,800 枚の 360◦ 画像と 22,956 文の説明文からなる QuIC-360◦ を構築した．QuIC-360◦ による再学習で，360◦ 画像に対してクエリを用いることで説明文生成の制御性・多様性が高まることが確認された．",
    "pdf_link": "/assets/papers/2023_d_quic360.pdf",
    "code_link": "https://github.com/Silviase/QuIC-360",
    "type": "domestic",
    "bibtex": "@inproceedings{maeda2023quic360,\n  title={QuIC-360◦: 360◦ 画像に対するクエリ指向画像説明文生成のためのデータセット構築},\n  author={前田 航希 and 栗田 修平 and 宮西 大樹},\n  booktitle={言語処理学会第29回年次大会 (NLP2023)},\n  pages={3013--3018},\n  year={2023},\n  address={東京}\n}\n",
    "url": "/papers/2023-d-quic360/"
  }
  
  ,
  
  {
    "id": null,
    "title": "DueT: 視覚・言語のDual-adapter Tuningによる基盤モデル",
    "venue": "言語処理学会第29回年次大会 (NLP2023)",
    "description": "対照学習により構築する視覚・言語の基盤モデル CLIP の新たな転移学習方法として DueT を提案する．DueT は単一モーダルのコーパスで事前学習されたモデルにより画像・テキストエンコーダを初期化して固定し，両エンコーダに追加したゲート機構付のアダプタのみを学習する．英語・日本語ドメインの 0-shot 画像・テキスト検索において，単純な ﬁne-tuning や画像エンコーダのみ転移・固定する従来手法に比べ，提案手法が精度やパラメータ効率性の観点で優れていたことを報告する．",
    "pdf_link": "/assets/papers/2023_d_duet.pdf",
    "code_link": "",
    "type": "domestic",
    "bibtex": "@inproceedings{nishida2023duet,\n  title={DueT: 視覚・言語のDual-adapter Tuningによる基盤モデル},\n  author={西田 京介 and 長谷川 拓 and 前田 航希 and 齋藤 邦子},\n  booktitle={言語処理学会第29回年次大会 (NLP2023)},\n  pages={1586--1591},\n  year={2023},\n  address={東京}\n}\n",
    "url": "/papers/2023-d-duet/"
  }
  
  ,
  
  {
    "id": null,
    "title": "IMPARA: Impact-Based Metric for GEC Using Parallel Data",
    "venue": "Proceedings of the 29th International Conference on Computational Linguistics (COLING 2022)",
    "description": "Proposal of a new impact-based metric for grammatical error correction using parallel datasets.",
    "pdf_link": "https://aclanthology.org/2022.coling-1.316.pdf",
    "code_link": "https://github.com/Silviase/IMPARA",
    "type": "international",
    "bibtex": "@inproceedings{maeda2022impara,\n  title = {IMPARA: Impact-Based Metric for GEC Using Parallel Data},\n  author = {Koki Maeda and Masahiro Kaneko and Naoaki Okazaki},\n  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics (COLING 2022)},\n  pages = {3578--3588},\n  year = {2022},\n  address = {Gyeongju, Republic of Korea},\n  publisher = {International Committee on Computational Linguistics}\n}\n",
    "url": "/papers/2022-impara/"
  }
  
  ,
  
  {
    "id": null,
    "title": "IMPARA: パラレルデータにおける修正の影響度に基づいた文法誤り訂正の自動評価法",
    "venue": "言語処理学会第28回年次大会 (NLP2022)",
    "description": "文法誤り訂正（Grammatical Error Correction; GEC）の自動評価は，低コストかつ定量的な評価に不可欠である．しかし，既存の GEC 自動評価手法は評価時に複数の参照文を必要としたり，評価モデルの学習に特化した訓練データが必要になるなど，自動評価の実現のためのデータ作成コストが高いという難点がある．本稿では，誤文と正文の組からなるパラレルデータのみを用い，修正の影響度を考慮しながら GEC の評価尺度を学習する手法である IMPARA を提案する．提案手法は GEC の自動評価におけるデータ作成コストを大幅に軽減しつつ，人手評価との相関において既存手法と同等以上の性能を示した．また，評価尺度を学習するパラレルデータを変更することで，異なるドメインや訂正スタイルに適合した評価を実現できることを実験的に示した．",
    "pdf_link": "/assets/papers/2022_d_impara.pdf",
    "code_link": "https://github.com/Silviase/IMPARA",
    "type": "domestic",
    "bibtex": "@inproceedings{maeda2022impara,\n  title={IMPARA: パラレルデータにおける修正の影響度に基づいた文法誤り訂正の自動評価法},\n  author={前田 航希 and 金子 正弘 and 岡崎 直観},\n  booktitle={言語処理学会第28回年次大会 (NLP2022)},\n  pages={328--333},\n  year={2022},\n  address={東京}\n}\n",
    "url": "/papers/2022-d-impara/"
  }
  
  
  
]