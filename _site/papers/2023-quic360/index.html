<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="icon" href="/assets/favicon.svg" type="image/svg+xml" />
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
<link rel="stylesheet" href="/assets/css/main.css" />
  
<meta name="citation_title" content="Query-based Image Captioning from Multi-context 360-degree Images" />
 
<meta name="citation_author" content="Koki Maeda" />

<meta name="citation_author" content="Shuhei Kurita" />

<meta name="citation_author" content="Taiki Miyanishi" />

<meta name="citation_author" content="Naoaki Okazaki" />
  
<meta name="citation_conference_title" content="Findings of the Association for Computational Linguistics: EMNLP 2023" />
  
<meta name="citation_publication_date" content="2023/12/06" />
<meta name="citation_date" content="2023" />
 
<meta name="citation_firstpage" content="6940" />
 
<meta name="citation_lastpage" content="6954" />
 
<meta name="citation_abstract_html_url" content="https://silviase.github.io/papers/2023-quic360/" />
  
<meta name="citation_pdf_url" content="https://aclanthology.org/2023.findings-emnlp.463.pdf" />

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Query-based Image Captioning from Multi-context 360-degree Images | Koki Maeda</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Query-based Image Captioning from Multi-context 360-degree Images" />
<meta name="author" content="Koki Maeda" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A novel image captioning approach that leverages queries and multi-context 360-degree imagery." />
<meta property="og:description" content="A novel image captioning approach that leverages queries and multi-context 360-degree imagery." />
<link rel="canonical" href="https://silviase.github.io/papers/2023-quic360/" />
<meta property="og:url" content="https://silviase.github.io/papers/2023-quic360/" />
<meta property="og:site_name" content="Koki Maeda" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-06T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Query-based Image Captioning from Multi-context 360-degree Images" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Koki Maeda"},"dateModified":"2023-12-06T00:00:00+09:00","datePublished":"2023-12-06T00:00:00+09:00","description":"A novel image captioning approach that leverages queries and multi-context 360-degree imagery.","headline":"Query-based Image Captioning from Multi-context 360-degree Images","mainEntityOfPage":{"@type":"WebPage","@id":"https://silviase.github.io/papers/2023-quic360/"},"url":"https://silviase.github.io/papers/2023-quic360/"}</script>
<!-- End Jekyll SEO tag -->


  </head>
  <body class="layout-paper">
    <div class="page-shell">
      <header class="site-header">
  <div class="container">
    <div class="site-branding">
      <a class="site-title" href="/">Koki Maeda</a>
      
      <p class="site-tagline">Doctoral student exploring multimodal vision-and-language systems, evaluation metrics, and context-aware captioning.</p>
      
    </div>
    <nav class="site-nav">
           
      <a href="/" class="">Home</a>
         
      <a href="/cv/" class="">CV</a>
         
      <a href="/papers/" class="active">Papers</a>
         
      <a href="/blog/" class="">Blog</a>
      
    </nav>
  </div>
</header>

      <main class="site-main"><article class="paper-detail">
  <div class="container">
    <nav class="back-nav">
      <a href="/papers/">‚Üê Back to Papers</a>
    </nav>
    <header class="paper-header">
      <h1 class="paper-title">Query-based Image Captioning from Multi-context 360-degree Images</h1>
      
      <p class="paper-authors">
        
        <span class="paper-author">Koki Maeda</span>,  
        <span class="paper-author">Shuhei Kurita</span>,  
        <span class="paper-author">Taiki Miyanishi</span>,  
        <span class="paper-author">Naoaki Okazaki</span> 
      </p>
      
      <p class="paper-publication">
         Findings of the Association for Computational Linguistics: EMNLP 2023  
        ¬∑ December 2023 
      </p>
      
      <p class="paper-summary">A novel image captioning approach that leverages queries and multi-context 360-degree imagery.</p>
      
      <div class="paper-actions">
           
        <a class="button" href="https://aclanthology.org/2023.findings-emnlp.463.pdf" target="_blank" rel="noopener">
          <span aria-hidden="true">üìÑ</span> PDF
        </a>
         
        <a class="button" href="https://github.com/Silviase/QuIC-360" target="_blank" rel="noopener">
          <span aria-hidden="true">üíª</span> Code
        </a>
        
      </div>
    </header>

    
    <section class="paper-bibtex">
      <h2>BibTeX</h2>
      <pre><code>@inproceedings{maeda2023quic360,
  title = {Query-based Image Captioning from Multi-context 360-degree Images},
  author = {Koki Maeda and Shuhei Kurita and Taiki Miyanishi and Naoaki Okazaki},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages = {6940--6954},
  year = {2023},
  address = {Singapore},
  publisher = {Association for Computational Linguistics}
}</code></pre>
    </section>
    

    <section class="paper-content"><p><a href="/assets/papers/2023_quic360.pdf">PDF</a></p>

<h2 id="abstract">Abstract</h2>

<p>This paper introduces Query-based Image Captioning (QuIC) for 360-degree images, where a query (words or short phrases) specifies the context to describe. This task is more challenging than conventional image captioning, as it requires fine-grained scene understanding to select content consistent with the user‚Äôs intent based on the query. We constructed a dataset comprising 3,940 360-degree images and 18,459 pairs of queries and captions annotated manually. Experiments demonstrate that fine-tuning image captioning models on our dataset can generate more diverse and controllable captions from multiple contexts of 360-degree images.</p>

<h2 id="methodology">Methodology</h2>

<p>The proposed QuIC task involves generating captions for 360-degree images based on user-specified queries. We developed a dataset with manually annotated query-caption pairs to facilitate this task. The methodology includes fine-tuning existing image captioning models on this dataset to enable them to produce contextually relevant captions corresponding to the provided queries.</p>

<h2 id="experimental-results">Experimental Results</h2>

<p>Experiments show that models fine-tuned on our dataset can generate more diverse and controllable captions for 360-degree images. The results indicate that the QuIC approach effectively captures multiple contexts within 360-degree imagery, aligning with user-specified queries.</p>

<h2 id="key-contributions">Key Contributions</h2>

<ul>
  <li>
    <p><strong>Introduction of QuIC Task</strong>: We propose the novel task of Query-based Image Captioning for 360-degree images, addressing the challenge of describing specific contexts within panoramic imagery.</p>
  </li>
  <li>
    <p><strong>Dataset Construction</strong>: We created a dataset consisting of 3,940 360-degree images and 18,459 manually annotated query-caption pairs to support research in this area.</p>
  </li>
  <li>
    <p><strong>Model Fine-tuning</strong>: We demonstrate that fine-tuning image captioning models on our dataset enables the generation of diverse and contextually relevant captions based on user queries.</p>
  </li>
</ul>

<h2 id="conclusion-and-future-work">Conclusion and Future Work</h2>

<p>The QuIC task presents a promising direction for enhancing image captioning models to handle user-specified contexts within 360-degree images. Future work may explore the application of QuIC to other types of imagery and investigate more sophisticated models capable of understanding and generating captions for complex scenes.</p>
</section>
  </div>
</article>
</main>
      <footer class="site-footer">
  <div class="container">
    <p>¬© 2025 Koki Maeda ¬∑ Built with Jekyll</p>
  </div>
</footer>

    </div>
    <script src="/assets/js/site.js"></script>
    
  </body>
</html>
