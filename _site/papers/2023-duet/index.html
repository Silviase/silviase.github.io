<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="icon" href="/assets/favicon.svg" type="image/svg+xml" />
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
<link rel="stylesheet" href="/assets/css/main.css" />
  
<meta name="citation_title" content="DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning" />
 
<meta name="citation_author" content="Taku Hasegawa" />

<meta name="citation_author" content="Kyosuke Nishida" />

<meta name="citation_author" content="Koki Maeda" />

<meta name="citation_author" content="Kuniko Saito" />
  
<meta name="citation_conference_title" content="Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)" />
  
<meta name="citation_publication_date" content="2023/12/06" />
<meta name="citation_date" content="2023" />
 
<meta name="citation_firstpage" content="13607" />
 
<meta name="citation_lastpage" content="13624" />
 
<meta name="citation_abstract_html_url" content="https://silviase.github.io/papers/2023-duet/" />
  
<meta name="citation_pdf_url" content="https://aclanthology.org/2023.emnlp-main.839.pdf" />

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning | Koki Maeda</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning" />
<meta name="author" content="Taku Hasegawa" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Vision-language models like CLIP show strong zero-shot performance but struggle when fine-tuned on downstream tasks due to overfitting. This paper proposes DueT (Dual-adapter Tuning), which uses separate adapters for uni-modal and cross-modal features to prevent overfitting while maintaining the pre-trained knowledge. The method introduces contrastive learning between adapted and original features, achieving state-of-the-art results on multiple vision-language benchmarks. DueT demonstrates significant improvements over existing adapter-based methods, particularly in few-shot scenarios where overfitting is most problematic." />
<meta property="og:description" content="Vision-language models like CLIP show strong zero-shot performance but struggle when fine-tuned on downstream tasks due to overfitting. This paper proposes DueT (Dual-adapter Tuning), which uses separate adapters for uni-modal and cross-modal features to prevent overfitting while maintaining the pre-trained knowledge. The method introduces contrastive learning between adapted and original features, achieving state-of-the-art results on multiple vision-language benchmarks. DueT demonstrates significant improvements over existing adapter-based methods, particularly in few-shot scenarios where overfitting is most problematic." />
<link rel="canonical" href="https://silviase.github.io/papers/2023-duet/" />
<meta property="og:url" content="https://silviase.github.io/papers/2023-duet/" />
<meta property="og:site_name" content="Koki Maeda" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-06T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Taku Hasegawa"},"dateModified":"2023-12-06T00:00:00+09:00","datePublished":"2023-12-06T00:00:00+09:00","description":"Vision-language models like CLIP show strong zero-shot performance but struggle when fine-tuned on downstream tasks due to overfitting. This paper proposes DueT (Dual-adapter Tuning), which uses separate adapters for uni-modal and cross-modal features to prevent overfitting while maintaining the pre-trained knowledge. The method introduces contrastive learning between adapted and original features, achieving state-of-the-art results on multiple vision-language benchmarks. DueT demonstrates significant improvements over existing adapter-based methods, particularly in few-shot scenarios where overfitting is most problematic.","headline":"DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning","mainEntityOfPage":{"@type":"WebPage","@id":"https://silviase.github.io/papers/2023-duet/"},"url":"https://silviase.github.io/papers/2023-duet/"}</script>
<!-- End Jekyll SEO tag -->


  </head>
  <body class="layout-paper">
    <div class="page-shell">
      <header class="site-header">
  <div class="container">
    <div class="site-branding">
      <a class="site-title" href="/">Koki Maeda</a>
      
      <p class="site-tagline">Doctoral student exploring multimodal vision-and-language systems, evaluation metrics, and context-aware captioning.</p>
      
    </div>
    <nav class="site-nav">
           
      <a href="/" class="">Home</a>
         
      <a href="/cv/" class="">CV</a>
         
      <a href="/papers/" class="active">Papers</a>
         
      <a href="/blog/" class="">Blog</a>
      
    </nav>
  </div>
</header>

      <main class="site-main"><article class="paper-detail">
  <div class="container">
    <nav class="back-nav">
      <a href="/papers/">‚Üê Back to Papers</a>
    </nav>
    <header class="paper-header">
      <h1 class="paper-title">DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning</h1>
      
      <p class="paper-authors">
        
        <span class="paper-author">Taku Hasegawa</span>,  
        <span class="paper-author">Kyosuke Nishida</span>,  
        <span class="paper-author">Koki Maeda</span>,  
        <span class="paper-author">Kuniko Saito</span> 
      </p>
      
      <p class="paper-publication">
         Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)  
        ¬∑ December 2023 
      </p>
      
      <p class="paper-summary">Vision-language models like CLIP show strong zero-shot performance but struggle when fine-tuned on downstream tasks due to overfitting. This paper proposes DueT (Dual-adapter Tuning), which uses separate adapters for uni-modal and cross-modal features to prevent overfitting while maintaining the pre-trained knowledge. The method introduces contrastive learning between adapted and original features, achieving state-of-the-art results on multiple vision-language benchmarks. DueT demonstrates significant improvements over existing adapter-based methods, particularly in few-shot scenarios where overfitting is most problematic.</p>
      
      <div class="paper-actions">
           
        <a class="button" href="https://aclanthology.org/2023.emnlp-main.839.pdf" target="_blank" rel="noopener">
          <span aria-hidden="true">üìÑ</span> PDF
        </a>
         
      </div>
    </header>

    
    <section class="paper-bibtex">
      <h2>BibTeX</h2>
      <pre><code>@inproceedings{hasegawa2023duet,
  title = {DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning},
  author = {Taku Hasegawa and Kyosuke Nishida and Koki Maeda and Kuniko Saito},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)},
  pages = {13607--13624},
  year = {2023},
  address = {Singapore},
  publisher = {Association for Computational Linguistics}
}</code></pre>
    </section>
    

    <section class="paper-content"><h2 id="abstract">Abstract</h2>

<p>This paper presents DueT, a novel transfer learning method for vision and language models built by contrastive learning. In DueT, adapters are inserted into the image and text encoders, which have been initialized using models pre-trained on uni-modal corpora and then frozen. By training only these adapters, DueT enables efficient learning with a reduced number of trainable parameters. Moreover, unlike traditional adapters, those in DueT are equipped with a gating mechanism, enabling effective transfer and connection of knowledge acquired from pre-trained uni-modal encoders while preventing catastrophic forgetting. We report that DueT outperformed simple fine-tuning, the conventional method fixing only the image encoder and training only the text encoder, and the LoRA-based adapter method in accuracy and parameter efficiency for 0-shot image and text retrieval in both English and Japanese domains.</p>

<h2 id="methodology">Methodology</h2>

<p>DueT employs a dual-adapter tuning approach where adapters with gating mechanisms are inserted into both the image and text encoders. These encoders are initialized with models pre-trained on uni-modal corpora and kept frozen during training. Only the adapters are trained, which allows for efficient learning with fewer parameters. The gating mechanism facilitates the effective transfer of knowledge from the pre-trained encoders while preventing catastrophic forgetting.</p>

<h2 id="experimental-results">Experimental Results</h2>

<p>Experiments conducted in both English and Japanese domains demonstrated that DueT outperforms several baseline methods. Specifically, DueT achieved higher accuracy and better parameter efficiency compared to simple fine-tuning, the conventional method of fixing only the image encoder and training only the text encoder, and the LoRA-based adapter method in zero-shot image and text retrieval tasks.</p>

<h2 id="key-contributions">Key Contributions</h2>

<ul>
  <li><strong>Transferability</strong>: DueT effectively transfers and connects knowledge acquired by pre-trained uni-modal encoders without catastrophic forgetting.</li>
  <li><strong>Parameter Efficiency</strong>: The method achieves superior performance in image-text pre-training tasks while utilizing fewer trainable parameters.</li>
</ul>

<h2 id="conclusion-and-future-work">Conclusion and Future Work</h2>

<p>DueT presents a promising approach to transfer learning in vision and language models by leveraging dual-adapter tuning with gating mechanisms. Future work may explore the application of DueT to other languages and domains, as well as its integration with different model architectures and training objectives.</p>
</section>
  </div>
</article>
</main>
      <footer class="site-footer">
  <div class="container">
    <p>¬© 2025 Koki Maeda ¬∑ Built with Jekyll</p>
  </div>
</footer>

    </div>
    <script src="/assets/js/site.js"></script>
    
  </body>
</html>
