<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="icon" href="/assets/favicon.svg" type="image/svg+xml" />
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
<link rel="stylesheet" href="/assets/css/main.css" />
  
<meta name="citation_title" content="Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model" />
 
<meta name="citation_author" content="Keito Sasagawa" />

<meta name="citation_author" content="Koki Maeda" />

<meta name="citation_author" content="Issa Sugiura" />

<meta name="citation_author" content="Shuhei Kurita" />

<meta name="citation_author" content="Naoaki Okazaki" />

<meta name="citation_author" content="Daisuke Kawahara" />
  
<meta name="citation_conference_title" content="Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)" />
  
<meta name="citation_publication_date" content="2025/06/01" />
<meta name="citation_date" content="2025" />
 
<meta name="citation_firstpage" content="1" />
 
<meta name="citation_lastpage" content="10" />
 
<meta name="citation_abstract_html_url" content="https://silviase.github.io/papers/2025-llmjp3vila/" />
  
<meta name="citation_pdf_url" content="https://arxiv.org/pdf/2410.22736" />

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model | Koki Maeda</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model" />
<meta name="author" content="Keito Sasagawa" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models." />
<meta property="og:description" content="Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models." />
<link rel="canonical" href="https://silviase.github.io/papers/2025-llmjp3vila/" />
<meta property="og:url" content="https://silviase.github.io/papers/2025-llmjp3vila/" />
<meta property="og:site_name" content="Koki Maeda" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-01T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Keito Sasagawa"},"dateModified":"2025-06-01T00:00:00+09:00","datePublished":"2025-06-01T00:00:00+09:00","description":"Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models.","headline":"Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model","mainEntityOfPage":{"@type":"WebPage","@id":"https://silviase.github.io/papers/2025-llmjp3vila/"},"url":"https://silviase.github.io/papers/2025-llmjp3vila/"}</script>
<!-- End Jekyll SEO tag -->


  </head>
  <body class="layout-paper">
    <div class="page-shell">
      <header class="site-header">
  <div class="container">
    <div class="site-branding">
      <a class="site-title" href="/">Koki Maeda</a>
      
      <p class="site-tagline">Doctoral student exploring multimodal vision-and-language systems, evaluation metrics, and context-aware captioning.</p>
      
    </div>
    <nav class="site-nav">
           
      <a href="/" class="">Home</a>
         
      <a href="/cv/" class="">CV</a>
         
      <a href="/papers/" class="active">Papers</a>
         
      <a href="/blog/" class="">Blog</a>
      
    </nav>
  </div>
</header>

      <main class="site-main"><article class="paper-detail">
  <div class="container">
    <nav class="back-nav">
      <a href="/papers/">‚Üê Back to Papers</a>
    </nav>
    <header class="paper-header">
      <h1 class="paper-title">Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model</h1>
      
      <p class="paper-authors">
        
        <span class="paper-author">Keito Sasagawa</span>,  
        <span class="paper-author">Koki Maeda</span>,  
        <span class="paper-author">Issa Sugiura</span>,  
        <span class="paper-author">Shuhei Kurita</span>,  
        <span class="paper-author">Naoaki Okazaki</span>,  
        <span class="paper-author">Daisuke Kawahara</span> 
      </p>
      
      <p class="paper-publication">
         Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)  
        ¬∑ June 2025 
      </p>
      
      <p class="paper-summary">Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models.</p>
      
      <div class="paper-actions">
           
        <a class="button" href="https://arxiv.org/pdf/2410.22736" target="_blank" rel="noopener">
          <span aria-hidden="true">üìÑ</span> PDF
        </a>
         
        <a class="button" href="https://huggingface.co/llm-jp/llm-jp-3-vila-14b" target="_blank" rel="noopener">
          <span aria-hidden="true">üíª</span> Code
        </a>
        
      </div>
    </header>

    
    <section class="paper-bibtex">
      <h2>BibTeX</h2>
      <pre><code>@inproceedings{sasagawa2025llmjp3vila,
  title = {Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model},
  author = {Keito Sasagawa and Koki Maeda and Issa Sugiura and Shuhei Kurita and Naoaki Okazaki and Daisuke Kawahara},
  booktitle = {Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)},
  year = {2025},
  address = {Albuquerque, USA},
  publisher = {Association for Computational Linguistics}
}</code></pre>
    </section>
    

    <section class="paper-content"><h2 id="abstract">Abstract</h2>

<p>This study addresses the scarcity of multimodal datasets for non-English languages, specifically focusing on Japanese, for visual language models (VLMs). We present an efficient method for rapidly creating comprehensive Japanese multimodal datasets. This involves extracting Japanese image-text pairs from web archives and generating instruction data directly from images using established vision-language models (VLMs). Our datasets significantly enhance the alignment between visual and textual content compared to machine-translated alternatives. Experimental evaluations demonstrate that VLMs trained on our datasets achieve superior accuracy, promoting regional localization and cultural accuracy in multimodal tasks.</p>

<h2 id="methodology">Methodology</h2>

<p>We introduce a streamlined pipeline for constructing multimodal datasets from scratch:</p>

<ul>
  <li><strong>Pretraining Data:</strong> Curated from web archives, we extracted 6 million Japanese image-text pairs and interleaved data, employing extensive filtering techniques to ensure quality and relevance.</li>
  <li><strong>Instruction Tuning Data:</strong> Leveraging existing VLMs (e.g., LLaVA), we generated a rich, contextually accurate dataset directly from Japanese images, resulting in 369K samples across various instruction types.</li>
  <li><strong>Architecture:</strong> Our llm-jp-3 VILA 14B model combines a vision transformer (SigLIP) with a Japanese language model (llm-jp-3-13b-instruct), connected via a two-layer MLP projector.</li>
</ul>

<h2 id="experimental-results">Experimental Results</h2>

<p>llm-jp-3 VILA 14B demonstrated state-of-the-art performance on Japanese benchmarks including Heron-Bench, JA-VLM-Bench-In-the-Wild, and JA-VG-VQA-500. Notably:</p>

<ul>
  <li>Achieved superior scores in LLM-as-a-Judge evaluations, indicating excellent multimodal understanding and alignment.</li>
  <li>Surpassed GPT-4o performance on certain benchmarks, underscoring the effectiveness of native Japanese datasets over translated ones.</li>
</ul>

<h2 id="key-contributions">Key Contributions</h2>

<ul>
  <li>Developed a novel method for rapidly creating high-quality Japanese multimodal datasets.</li>
  <li>Demonstrated the limitations of relying on translated datasets for non-English languages in multimodal contexts.</li>
  <li>Provided llm-jp-3 VILA 14B, a robust Japanese VLM capable of cultural and contextual accuracy in multimodal tasks.</li>
</ul>

<h2 id="conclusion-and-future-work">Conclusion and Future Work</h2>

<p>Our methodology significantly enriches resources for Japanese VLMs, addressing the critical gap in non-English multimodal datasets. Future research includes expanding dataset diversity and enhancing dataset quality through advanced filtering and synthesis techniques.</p>
</section>
  </div>
</article>
</main>
      <footer class="site-footer">
  <div class="container">
    <p>¬© 2025 Koki Maeda ¬∑ Built with Jekyll</p>
  </div>
</footer>

    </div>
    <script src="/assets/js/site.js"></script>
    
  </body>
</html>
