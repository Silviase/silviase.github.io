<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="utf-8">
    <meta name="citation_title" content="Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model" />
    
    <meta name="citation_author" content="Keito Sasagawa" />
    
    <meta name="citation_author" content="Koki Maeda" />
    
    <meta name="citation_author" content="Issa Sugiura" />
    
    <meta name="citation_author" content="Shuhei Kurita" />
    
    <meta name="citation_author" content="Naoaki Okazaki" />
    
    <meta name="citation_author" content="Daisuke Kawahara" />
    
    <meta name="citation_conference_title" content="Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)" />
    <meta name="citation_date" content="2025" />
    <meta name="citation_publication_date" content="2025/06/01" />
    
    <meta name="citation_firstpage" content="1" />
    
    
    <meta name="citation_lastpage" content="10" />
    
    <meta name="citation_abstract_html_url" content="/papers/2025-llmjp3vila/">
    
    <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2410.22736" />
    
    <title>Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="/style.css">
    <script>
      var shiftWindow = function() { scrollBy(0, -50) };
      window.addEventListener("hashchange", shiftWindow);
      function load() { if (window.location.hash) shiftWindow(); }
    </script>
</head>
<body>
    <div class="container">
        <header class="blog-header">
            <h1>Paper Detail</h1>
            <nav class="site-nav">
                <a href="/">Home</a>
                <a href="/papers.html" class="active">Papers</a>
                <a href="/blog.html">Blog</a>
            </nav>
        </header>

        <div class="paper-meta">
            <h1 class="title">Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model</h1>
            <div class="venue">Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)</div>
            <div class="description">Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models.</div>
            <div class="type">international</div>

            <div class="action-buttons">
                
                <a href="https://arxiv.org/pdf/2410.22736" class="pdf-link">
                    <i class="fas fa-file-pdf"></i>
                    PDF
                </a>
                
                
                <a href="https://huggingface.co/llm-jp/llm-jp-3-vila-14b" class="code-link">
                    <i class="fas fa-code"></i>
                    Code
                </a>
                
            </div>

            <pre class="bibtex">@inproceedings{sasagawa2025llmjp3vila,
  title = {Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model},
  author = {Keito Sasagawa and Koki Maeda and Issa Sugiura and Shuhei Kurita and Naoaki Okazaki and Daisuke Kawahara},
  booktitle = {Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)},
  year = {2025},
  address = {Albuquerque, USA},
  publisher = {Association for Computational Linguistics}
}
</pre>
        </div>

        <div class="paper-content"><h2 id="abstract">Abstract</h2>

<p>This study addresses the scarcity of multimodal datasets for non-English languages, specifically focusing on Japanese, for visual language models (VLMs). We present an efficient method for rapidly creating comprehensive Japanese multimodal datasets. This involves extracting Japanese image-text pairs from web archives and generating instruction data directly from images using established vision-language models (VLMs). Our datasets significantly enhance the alignment between visual and textual content compared to machine-translated alternatives. Experimental evaluations demonstrate that VLMs trained on our datasets achieve superior accuracy, promoting regional localization and cultural accuracy in multimodal tasks.</p>

<h2 id="methodology">Methodology</h2>

<p>We introduce a streamlined pipeline for constructing multimodal datasets from scratch:</p>

<ul>
  <li><strong>Pretraining Data:</strong> Curated from web archives, we extracted 6 million Japanese image-text pairs and interleaved data, employing extensive filtering techniques to ensure quality and relevance.</li>
  <li><strong>Instruction Tuning Data:</strong> Leveraging existing VLMs (e.g., LLaVA), we generated a rich, contextually accurate dataset directly from Japanese images, resulting in 369K samples across various instruction types.</li>
  <li><strong>Architecture:</strong> Our llm-jp-3 VILA 14B model combines a vision transformer (SigLIP) with a Japanese language model (llm-jp-3-13b-instruct), connected via a two-layer MLP projector.</li>
</ul>

<h2 id="experimental-results">Experimental Results</h2>

<p>llm-jp-3 VILA 14B demonstrated state-of-the-art performance on Japanese benchmarks including Heron-Bench, JA-VLM-Bench-In-the-Wild, and JA-VG-VQA-500. Notably:</p>

<ul>
  <li>Achieved superior scores in LLM-as-a-Judge evaluations, indicating excellent multimodal understanding and alignment.</li>
  <li>Surpassed GPT-4o performance on certain benchmarks, underscoring the effectiveness of native Japanese datasets over translated ones.</li>
</ul>

<h2 id="key-contributions">Key Contributions</h2>

<ul>
  <li>Developed a novel method for rapidly creating high-quality Japanese multimodal datasets.</li>
  <li>Demonstrated the limitations of relying on translated datasets for non-English languages in multimodal contexts.</li>
  <li>Provided llm-jp-3 VILA 14B, a robust Japanese VLM capable of cultural and contextual accuracy in multimodal tasks.</li>
</ul>

<h2 id="conclusion-and-future-work">Conclusion and Future Work</h2>

<p>Our methodology significantly enriches resources for Japanese VLMs, addressing the critical gap in non-English multimodal datasets. Future research includes expanding dataset diversity and enhancing dataset quality through advanced filtering and synthesis techniques.</p>
</div>
    </div>

    <footer class="footer">
        <div class="container">
            <p>© 2025 Koki Maeda • Last Updated: March 2025</p>
        </div>
    </footer>
</body>
</html>