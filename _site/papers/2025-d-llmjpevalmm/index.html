<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="icon" href="/assets/favicon.svg" type="image/svg+xml" />
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
<link rel="stylesheet" href="/assets/css/main.css" />
  
<meta name="citation_title" content="llm-jp-eval-mm: æ—¥æœ¬èªè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•è©•ä¾¡åŸºç›¤" />
 
<meta name="citation_author" content="å‰ç”°, èˆªå¸Œ" />

<meta name="citation_author" content="æ‰æµ¦, ä¸€ç‘³" />

<meta name="citation_author" content="å°ç”°, æ‚ ä»‹" />

<meta name="citation_author" content="æ —ç”°, ä¿®å¹³" />

<meta name="citation_author" content="å²¡å´, ç›´è¦³" />
  
<meta name="citation_conference_title" content="è¨€èªå‡¦ç†å­¦ä¼šç¬¬31å›å¹´æ¬¡å¤§ä¼š (NLP2025)" />
  
<meta name="citation_publication_date" content="2025/03/15" />
<meta name="citation_date" content="2025" />
 
<meta name="citation_firstpage" content="1303" />
 
<meta name="citation_lastpage" content="1308" />
 
<meta name="citation_abstract_html_url" content="https://silviase.github.io/papers/2025-d-llmjpevalmm/" />
  
<meta name="citation_pdf_url" content="https://silviase.github.io/assets/papers/2025_d_llmjpevalmm.pdf" />

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>llm-jp-eval-mm: æ—¥æœ¬èªè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•è©•ä¾¡åŸºç›¤ | Koki Maeda</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="llm-jp-eval-mm: æ—¥æœ¬èªè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•è©•ä¾¡åŸºç›¤" />
<meta name="author" content="å‰ç”°, èˆªå¸Œ" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models." />
<meta property="og:description" content="Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models." />
<link rel="canonical" href="https://silviase.github.io/papers/2025-d-llmjpevalmm/" />
<meta property="og:url" content="https://silviase.github.io/papers/2025-d-llmjpevalmm/" />
<meta property="og:site_name" content="Koki Maeda" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-03-15T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="llm-jp-eval-mm: æ—¥æœ¬èªè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•è©•ä¾¡åŸºç›¤" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"å‰ç”°, èˆªå¸Œ"},"dateModified":"2025-03-15T00:00:00+09:00","datePublished":"2025-03-15T00:00:00+09:00","description":"Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models.","headline":"llm-jp-eval-mm: æ—¥æœ¬èªè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•è©•ä¾¡åŸºç›¤","mainEntityOfPage":{"@type":"WebPage","@id":"https://silviase.github.io/papers/2025-d-llmjpevalmm/"},"url":"https://silviase.github.io/papers/2025-d-llmjpevalmm/"}</script>
<!-- End Jekyll SEO tag -->


  </head>
  <body class="layout-paper">
    <div class="page-shell">
      <header class="site-header">
  <div class="container">
    <div class="site-branding">
      <a class="site-title" href="/">Koki Maeda</a>
      
      <p class="site-tagline">Doctoral student exploring multimodal vision-and-language systems, evaluation metrics, and context-aware captioning.</p>
      
    </div>
    <nav class="site-nav">
           
      <a href="/" class="">Home</a>
         
      <a href="/cv/" class="">CV</a>
         
      <a href="/papers/" class="active">Papers</a>
         
      <a href="/blog/" class="">Blog</a>
      
    </nav>
  </div>
</header>

      <main class="site-main"><article class="paper-detail">
  <div class="container">
    <nav class="back-nav">
      <a href="/papers/">â† Back to Papers</a>
    </nav>
    <header class="paper-header">
      <h1 class="paper-title">llm-jp-eval-mm: æ—¥æœ¬èªè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•è©•ä¾¡åŸºç›¤</h1>
      
      <p class="paper-authors">
        
        <span class="paper-author">å‰ç”°, èˆªå¸Œ</span>,  
        <span class="paper-author">æ‰æµ¦, ä¸€ç‘³</span>,  
        <span class="paper-author">å°ç”°, æ‚ ä»‹</span>,  
        <span class="paper-author">æ —ç”°, ä¿®å¹³</span>,  
        <span class="paper-author">å²¡å´, ç›´è¦³</span> 
      </p>
      
      <p class="paper-publication">
         è¨€èªå‡¦ç†å­¦ä¼šç¬¬31å›å¹´æ¬¡å¤§ä¼š (NLP2025)  
        Â· March 2025 
      </p>
      
      <p class="paper-summary">Methodology for quickly constructing multimodal datasets tailored for Japanese vision-language models.</p>
      
      <div class="paper-actions">
           
        <a class="button" href="/assets/papers/2025_d_llmjpevalmm.pdf" target="_blank" rel="noopener">
          <span aria-hidden="true">ğŸ“„</span> PDF
        </a>
         
        <a class="button" href="https://github.com/llm-jp/llm-jp-eval-mm" target="_blank" rel="noopener">
          <span aria-hidden="true">ğŸ’»</span> Code
        </a>
        
      </div>
    </header>

    
    <section class="paper-bibtex">
      <h2>BibTeX</h2>
      <pre><code>@inproceedings{maeda2025llm-jp-eval-mm,
  author = {å‰ç”°, èˆªå¸Œ and æ‰æµ¦, ä¸€ç‘³ and å°ç”°, æ‚ ä»‹ and æ —ç”°, ä¿®å¹³ and å²¡å´, ç›´è¦³},
  month = mar,
  series = {è¨€èªå‡¦ç†å­¦ä¼šç¬¬31å›å¹´æ¬¡å¤§ä¼š (NLP2025)},
  title = {{llm-jp-eval-mm: æ—¥æœ¬èªè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•è©•ä¾¡åŸºç›¤}},
  year = {2025}
}</code></pre>
    </section>
    

    <section class="paper-content"><p><a href="/assets/papers/2025_d_llmjpevalmm.pdf">PDF</a></p>

<h2 id="abstract">Abstract</h2>

<p>The research rapidly advances vision-language models (VLM), but evaluation frameworks for Japanese vision-language (V&amp;L) tasks are still inadequate. This paper introduces llm-jp-eval-mm, a toolkit for systematically evaluating Japanese multimodal tasks. It unifies six existing Japanese multimodal tasks, enabling consistent benchmarking across multiple metrics. The toolkit is publicly available, aiming to facilitate continuous improvement and evaluation of Japanese VLMs.</p>

<h2 id="methodology">Methodology</h2>

<p>llm-jp-eval-mm standardizes input/output formats across diverse datasets, separating inference from evaluation. It uses a modular class structure (Task and Scorer) allowing easy extension and incorporation of new tasks and models. Evaluation setups are simplified, removing YAML-based configurations, thus enhancing maintainability and usability.</p>

<h2 id="experimental-results">Experimental Results</h2>

<p>Evaluations were performed on 13 publicly available Japanese and multilingual VLMs. Among Japanese-specialized models, llm-jp-3 VILA exhibited top performance across most tasks. For multilingual models, Qwen2-VL showed superior results, particularly in multi-image tasks, indicating advantages of advanced training strategies.</p>

<h2 id="key-contributions">Key Contributions</h2>

<ul>
  <li>A unified evaluation toolkit (llm-jp-eval-mm) for Japanese multimodal tasks.</li>
  <li>Simplified integration of new models/tasks via modular implementation.</li>
  <li>Comprehensive benchmarking of existing VLMs, highlighting performance gaps and advancements.</li>
</ul>

<h2 id="conclusion-and-future-work">Conclusion and Future Work</h2>

<p>llm-jp-eval-mm is the pioneering framework for systematic Japanese VLM evaluation, revealing both the advancement and remaining gaps compared to large-scale commercial models like GPT-4o. Future work includes expanding dataset diversity (e.g., specialized domains, image generation) and extending evaluations to other modalities such as 3D vision, audio, video, and Vision-Language-Action (VLA).</p>
</section>
  </div>
</article>
</main>
      <footer class="site-footer">
  <div class="container">
    <p>Â© 2025 Koki Maeda Â· Built with Jekyll</p>
  </div>
</footer>

    </div>
    <script src="/assets/js/site.js"></script>
    
  </body>
</html>
