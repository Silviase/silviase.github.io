<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="icon" href="/assets/favicon.svg" type="image/svg+xml" />
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
<link rel="stylesheet" href="/assets/css/main.css" />
  
<meta name="citation_title" content="COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset as a Vision-Language Benchmark" />
 
<meta name="citation_author" content="Koki Maeda" />

<meta name="citation_author" content="Tosho Hirasawa" />

<meta name="citation_author" content="Atsushi Hashimoto" />

<meta name="citation_author" content="Jun Harashima" />

<meta name="citation_author" content="Leszek Rybicki" />

<meta name="citation_author" content="Yusuke Fukasawa" />

<meta name="citation_author" content="Yoshitaka Ushiku" />
  
<meta name="citation_conference_title" content="Proceedings of The 18th European Conference on Computer Vision (ECCV 2024)" />
  
<meta name="citation_publication_date" content="2024/09/29" />
<meta name="citation_date" content="2024" />
 
<meta name="citation_firstpage" content="1" />
 
<meta name="citation_lastpage" content="16" />
 
<meta name="citation_abstract_html_url" content="https://silviase.github.io/papers/2024-comkitchens/" />
  
<meta name="citation_pdf_url" content="https://doi.org/10.32130/rdata.6.1" />

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset as a Vision-Language Benchmark | Koki Maeda</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset as a Vision-Language Benchmark" />
<meta name="author" content="Koki Maeda" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introducing a new vision-language dataset based on unedited overhead-view procedural cooking videos." />
<meta property="og:description" content="Introducing a new vision-language dataset based on unedited overhead-view procedural cooking videos." />
<link rel="canonical" href="https://silviase.github.io/papers/2024-comkitchens/" />
<meta property="og:url" content="https://silviase.github.io/papers/2024-comkitchens/" />
<meta property="og:site_name" content="Koki Maeda" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-09-29T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset as a Vision-Language Benchmark" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Koki Maeda"},"dateModified":"2024-09-29T00:00:00+09:00","datePublished":"2024-09-29T00:00:00+09:00","description":"Introducing a new vision-language dataset based on unedited overhead-view procedural cooking videos.","headline":"COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset as a Vision-Language Benchmark","mainEntityOfPage":{"@type":"WebPage","@id":"https://silviase.github.io/papers/2024-comkitchens/"},"url":"https://silviase.github.io/papers/2024-comkitchens/"}</script>
<!-- End Jekyll SEO tag -->


  </head>
  <body class="layout-paper">
    <div class="page-shell">
      <header class="site-header">
  <div class="container">
    <div class="site-branding">
      <a class="site-title" href="/">Koki Maeda</a>
      
      <p class="site-tagline">Doctoral student exploring multimodal vision-and-language systems, evaluation metrics, and context-aware captioning.</p>
      
    </div>
    <nav class="site-nav">
           
      <a href="/" class="">Home</a>
         
      <a href="/cv/" class="">CV</a>
         
      <a href="/papers/" class="active">Papers</a>
         
      <a href="/blog/" class="">Blog</a>
      
    </nav>
  </div>
</header>

      <main class="site-main"><article class="paper-detail">
  <div class="container">
    <nav class="back-nav">
      <a href="/papers/">‚Üê Back to Papers</a>
    </nav>
    <header class="paper-header">
      <h1 class="paper-title">COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset as a Vision-Language Benchmark</h1>
      
      <p class="paper-authors">
        
        <span class="paper-author">Koki Maeda</span>,  
        <span class="paper-author">Tosho Hirasawa</span>,  
        <span class="paper-author">Atsushi Hashimoto</span>,  
        <span class="paper-author">Jun Harashima</span>,  
        <span class="paper-author">Leszek Rybicki</span>,  
        <span class="paper-author">Yusuke Fukasawa</span>,  
        <span class="paper-author">Yoshitaka Ushiku</span> 
      </p>
      
      <p class="paper-publication">
         Proceedings of The 18th European Conference on Computer Vision (ECCV 2024)  
        ¬∑ September 2024 
      </p>
      
      <p class="paper-summary">Introducing a new vision-language dataset based on unedited overhead-view procedural cooking videos.</p>
      
      <div class="paper-actions">
           
        <a class="button" href="https://doi.org/10.32130/rdata.6.1" target="_blank" rel="noopener">
          <span aria-hidden="true">üìÑ</span> PDF
        </a>
         
        <a class="button" href="https://github.com/omron-sinicx/com_kitchens" target="_blank" rel="noopener">
          <span aria-hidden="true">üíª</span> Code
        </a>
        
      </div>
    </header>

    
    <section class="paper-bibtex">
      <h2>BibTeX</h2>
      <pre><code>@inproceedings{maeda2024comkitchens,
  title = {COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset as a Vision-Language Benchmark},
  author = {Koki Maeda and Tosho Hirasawa and Atsushi Hashimoto and Jun Harashima and Leszek Rybicki and Yusuke Fukasawa and Yoshitaka Ushiku},
  booktitle = {Proceedings of The 18th European Conference on Computer Vision (ECCV 2024)},
  year = {2024},
  address = {Milan, Italy},
  publisher = {ECCV}
}</code></pre>
    </section>
    

    <section class="paper-content"><h2 id="abstract">Abstract</h2>

<p>COM Kitchens introduces a novel vision-language dataset aimed at overcoming the limitations of current procedural video datasets, typically sourced from the web or ego-centric views. The dataset comprises unedited, overhead-view cooking videos captured using modern smartphones, providing environmental diversity and detailed annotations. We introduce two novel vision-language tasks: Online Recipe Retrieval (OnRR) and Dense Video Captioning on unedited Overhead-View videos (DVC-OV). Our benchmarks demonstrate the dataset‚Äôs capacity to highlight limitations in existing models, thus paving the way for future research.</p>

<h2 id="methodology">Methodology</h2>

<p>Videos were collected by distributing smartphones to participants, who recorded themselves preparing recipes from the Cookpad Recipe Dataset. We provided comprehensive guidelines to ensure data consistency and privacy. Each video includes detailed manual annotations, forming structured visual action graphs that link textual instructions with visual elements via bounding boxes and edges, clearly indicating actions and ingredient transformations throughout the cooking process.</p>

<h2 id="experimental-results">Experimental Results</h2>

<p>Experiments on baseline models such as UniVL, CLIP4Clip, and X-CLIP demonstrated significant challenges in handling OnRR and DVC-OV tasks. None of these models effectively solved the feasible recipe retrieval subtask, although fine-tuning significantly improved the accuracy of the recipe stage identification subtask. Dense video captioning experiments using models like Vid2Seq revealed a notable domain gap between web videos and unedited overhead-view videos, underscoring the dataset‚Äôs unique challenges.</p>

<h2 id="key-contributions">Key Contributions</h2>

<ul>
  <li>Introduction of an unedited, diverse fixed-view procedural cooking video dataset.</li>
  <li>Manual annotation of structured visual action graphs for detailed event tracking.</li>
  <li>Proposal of novel vision-language tasks tailored for practical application scenarios.</li>
  <li>Benchmark analysis revealing critical limitations and gaps in current state-of-the-art models.</li>
</ul>

<h2 id="conclusion-and-future-work">Conclusion and Future Work</h2>

<p>The COM Kitchens dataset addresses critical gaps in procedural video understanding by providing a comprehensive resource for the vision-language community. Future work includes expanding the dataset, exploring advanced models that leverage the unique structure of visual action graphs, and developing robust applications for practical, real-world scenarios.</p>
</section>
  </div>
</article>
</main>
      <footer class="site-footer">
  <div class="container">
    <p>¬© 2025 Koki Maeda ¬∑ Built with Jekyll</p>
  </div>
</footer>

    </div>
    <script src="/assets/js/site.js"></script>
    
  </body>
</html>
